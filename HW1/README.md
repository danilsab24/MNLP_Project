# Project Overview
## Folder Structure

### Home Page
- Two `.ipynb` notebooks used to extract additional features and train the models.

### dataset
- Contains `.csv` files generated by the `NO_LM.ipynb` notebook.

### trained_model
- Contains all trained models, both:
  - Non-LM-based
  - LM-based (saved in the subfolder `model_roBERTa_full`)

### other_LLM
- Contains additional language models we tested:
  - BERT
  - DeBERTa base
  - RoBERTa base
  - RoBERTa XML

### Report
- `The_Tokenizers_report_hw1.pdf`: A brief report describing the work done in this homework, including comments on the results.

### Results
- Contain the results obtained from the LLM method (RoBERTa-large) and no LM method

## How to run the code
### Run on Colab 
Everything is done, you need just enough GPU for the run of `roBERTa-LARGE.ipynb`
### Run Local
Before run any files you need to install the libraries using `requirements.txt` file; if you are in vs-code you need just write this line of code on terminal:
```bash
pip install -r .\requirements.txt
```
- `NO_LM.ipynb` for 