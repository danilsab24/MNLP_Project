# Project Overview

## Folder Structure

### Home Page
- Contains two `.ipynb` notebooks responsible for feature extraction and model training.

### `dataset`
- Includes `.csv` files generated by the `NO_LM.ipynb` notebook.

### `trained_model`
- Stores all trained models, including:
  - Models not based on language models (non-LM)
  - Language model-based models (stored in the `model_roBERTa_full` subdirectory)

### `other_LLM`
- Contains additional pre-trained language models evaluated during the project:
  - BERT
  - DeBERTa (base)
  - RoBERTa (base)
  - RoBERTa (XML)

### `Report`
- `The_Tokenizers_report_hw1.pdf`: A concise report summarizing the work completed for this assignment, including an analysis of the results.

### `Results`
- Contains the results obtained from both the language model-based method (RoBERTa-large) and the non-language-model method.

## Execution Instructions

### Running on Google Colab
All setup is complete. You only need to ensure that a GPU with sufficient memory is available to execute the `roBERTa-LARGE.ipynb` notebook.

### Running Locally
Before executing any notebook, it is necessary to install the required dependencies listed in the `requirements.txt` file. If using VS Code, run the following command in the terminal:
```bash
pip install -r .\requirements.txt
```

- `roBERTa-LARGE.ipynb`: Once the dependencies are installed, this notebook can be run without further modification.
- `NO_LM.ipynb`: After installing the dependencies, it is important to comment out the following lines before running the notebook:
```python
!pip install datasets
!pip install wikidata
!pip install SPARQLWrapper
```

## Authors
The following contributors participated in the development of this project:
- Luca Conti – conti.1702084@studenti.uniroma1.it  
- Riccardo D'Aguanno – daguanno.2172315@studenti.uniroma1.it  
- Daniele Sabatini – sabatini.1890300@studenti.uniroma1.it
